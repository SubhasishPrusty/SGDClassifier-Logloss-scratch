{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "I2S-uFqwSvmg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Compare Loss and weight of SGDClassifier with log loss and our model using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So first we create a dataset with binary classification. Then we use it on SGDClassifier and print the loss after epoch and then the final weight vector and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FUxLkBjISvmr"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xexp5GYNSvmz",
    "outputId": "07eea558-f9c2-4457-8a71-a60a48228677"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54vJVc_KSvm9"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9pKAn1-ASvm_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "r97pFTgrSvnE"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jykLIXZNSvnJ",
    "outputId": "a190deab-5605-4786-b4f6-a59c9300ff77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0-M6oXASvnO"
   },
   "source": [
    " ## SGDClassifier with Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sShoMeocSvnP"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "gm6wi8L2SvnU",
    "outputId": "be306b64-dadd-4efd-ef20-619b5f7d3c58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='constant', loss='log', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "       tol=0.001, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float  lambda\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate (eta0) = 0.0001\n",
    "lamda/alpha = 0.0001\n",
    "\n",
    "The learning rate is also kept constant i.e. it wont change with each time the weight are optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "Q4WFoxgASvnc",
    "outputId": "56d8a216-b3f2-415e-d57e-5a8b00c96ffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.76, NNZs: 15, Bias: -0.314605, T: 37500, Avg. loss: 0.455801\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.92, NNZs: 15, Bias: -0.469578, T: 75000, Avg. loss: 0.394737\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580452, T: 112500, Avg. loss: 0.385561\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.660824, T: 150000, Avg. loss: 0.382161\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.717218, T: 187500, Avg. loss: 0.380474\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.761816, T: 225000, Avg. loss: 0.379481\n",
      "Total training time: 0.15 seconds.\n",
      "Convergence after 6 epochs took 0.15 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='constant', loss='log', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "       tol=0.001, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "7WaVxhGpSvnj",
    "outputId": "4597c980-81af-40ad-ca1d-2ff327025a64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.41177431,  0.18416782, -0.13895073,  0.33572511, -0.18423237,\n",
       "          0.5494352 , -0.45213692, -0.08857465,  0.21536661,  0.17351757,\n",
       "          0.18480827,  0.00443463, -0.07033001,  0.33683181,  0.02004129]]),\n",
       " (1, 15),\n",
       " array([-0.76181561]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Su9e8fRLSvno"
   },
   "source": [
    " ## Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcz5_UqCSvnq"
   },
   "source": [
    "**For better clarity we havent added a column of ones to Xtrain and rather kept the weights vector and bias separate.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0voVRQyp_NHh"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Hz5KIxr9X7dJ"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "OBoYVMOulyt9",
    "outputId": "466a774d-241a-4f4b-b508-312e3349e133"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.266454</td>\n",
       "      <td>0.207401</td>\n",
       "      <td>0.207484</td>\n",
       "      <td>-0.491013</td>\n",
       "      <td>-0.700210</td>\n",
       "      <td>-0.294138</td>\n",
       "      <td>0.273313</td>\n",
       "      <td>0.695944</td>\n",
       "      <td>-0.030006</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.609432</td>\n",
       "      <td>-1.110552</td>\n",
       "      <td>-0.528438</td>\n",
       "      <td>-0.511179</td>\n",
       "      <td>0.231617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.126825</td>\n",
       "      <td>1.970479</td>\n",
       "      <td>1.874263</td>\n",
       "      <td>1.762062</td>\n",
       "      <td>1.649456</td>\n",
       "      <td>1.723327</td>\n",
       "      <td>2.050374</td>\n",
       "      <td>1.986726</td>\n",
       "      <td>4.385574</td>\n",
       "      <td>2.094178</td>\n",
       "      <td>4.066106</td>\n",
       "      <td>3.406985</td>\n",
       "      <td>2.612897</td>\n",
       "      <td>2.073009</td>\n",
       "      <td>2.525177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-8.360184</td>\n",
       "      <td>-8.791185</td>\n",
       "      <td>-7.383914</td>\n",
       "      <td>-7.692214</td>\n",
       "      <td>-7.729431</td>\n",
       "      <td>-7.750905</td>\n",
       "      <td>-8.661642</td>\n",
       "      <td>-7.848835</td>\n",
       "      <td>-23.924167</td>\n",
       "      <td>-8.678568</td>\n",
       "      <td>-17.056269</td>\n",
       "      <td>-18.299831</td>\n",
       "      <td>-13.340440</td>\n",
       "      <td>-8.971211</td>\n",
       "      <td>-12.804785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.162846</td>\n",
       "      <td>-1.090813</td>\n",
       "      <td>-1.084520</td>\n",
       "      <td>-1.675401</td>\n",
       "      <td>-1.805095</td>\n",
       "      <td>-1.430182</td>\n",
       "      <td>-1.103160</td>\n",
       "      <td>-0.607170</td>\n",
       "      <td>-2.989116</td>\n",
       "      <td>-1.427887</td>\n",
       "      <td>-1.924670</td>\n",
       "      <td>-3.311911</td>\n",
       "      <td>-2.213970</td>\n",
       "      <td>-1.899357</td>\n",
       "      <td>-1.422223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.241495</td>\n",
       "      <td>0.235629</td>\n",
       "      <td>0.183688</td>\n",
       "      <td>-0.543458</td>\n",
       "      <td>-0.705345</td>\n",
       "      <td>-0.354602</td>\n",
       "      <td>0.275627</td>\n",
       "      <td>0.705302</td>\n",
       "      <td>0.027739</td>\n",
       "      <td>-0.018822</td>\n",
       "      <td>0.635744</td>\n",
       "      <td>-1.163694</td>\n",
       "      <td>-0.598621</td>\n",
       "      <td>-0.497557</td>\n",
       "      <td>0.196780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.683281</td>\n",
       "      <td>1.501122</td>\n",
       "      <td>1.461999</td>\n",
       "      <td>0.656996</td>\n",
       "      <td>0.401761</td>\n",
       "      <td>0.784513</td>\n",
       "      <td>1.660565</td>\n",
       "      <td>2.002337</td>\n",
       "      <td>2.912834</td>\n",
       "      <td>1.382269</td>\n",
       "      <td>3.155235</td>\n",
       "      <td>1.049781</td>\n",
       "      <td>1.068512</td>\n",
       "      <td>0.908975</td>\n",
       "      <td>1.822896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.017564</td>\n",
       "      <td>8.879740</td>\n",
       "      <td>8.065992</td>\n",
       "      <td>7.255772</td>\n",
       "      <td>6.487590</td>\n",
       "      <td>7.808481</td>\n",
       "      <td>8.361272</td>\n",
       "      <td>10.323580</td>\n",
       "      <td>19.131734</td>\n",
       "      <td>10.206260</td>\n",
       "      <td>18.604384</td>\n",
       "      <td>15.941515</td>\n",
       "      <td>15.278376</td>\n",
       "      <td>7.844041</td>\n",
       "      <td>14.235675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000   \n",
       "mean       0.266454      0.207401      0.207484     -0.491013     -0.700210   \n",
       "std        2.126825      1.970479      1.874263      1.762062      1.649456   \n",
       "min       -8.360184     -8.791185     -7.383914     -7.692214     -7.729431   \n",
       "25%       -1.162846     -1.090813     -1.084520     -1.675401     -1.805095   \n",
       "50%        0.241495      0.235629      0.183688     -0.543458     -0.705345   \n",
       "75%        1.683281      1.501122      1.461999      0.656996      0.401761   \n",
       "max       11.017564      8.879740      8.065992      7.255772      6.487590   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000   \n",
       "mean      -0.294138      0.273313      0.695944     -0.030006      0.006645   \n",
       "std        1.723327      2.050374      1.986726      4.385574      2.094178   \n",
       "min       -7.750905     -8.661642     -7.848835    -23.924167     -8.678568   \n",
       "25%       -1.430182     -1.103160     -0.607170     -2.989116     -1.427887   \n",
       "50%       -0.354602      0.275627      0.705302      0.027739     -0.018822   \n",
       "75%        0.784513      1.660565      2.002337      2.912834      1.382269   \n",
       "max        7.808481      8.361272     10.323580     19.131734     10.206260   \n",
       "\n",
       "                 10            11            12            13            14  \n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000  \n",
       "mean       0.609432     -1.110552     -0.528438     -0.511179      0.231617  \n",
       "std        4.066106      3.406985      2.612897      2.073009      2.525177  \n",
       "min      -17.056269    -18.299831    -13.340440     -8.971211    -12.804785  \n",
       "25%       -1.924670     -3.311911     -2.213970     -1.899357     -1.422223  \n",
       "50%        0.635744     -1.163694     -0.598621     -0.497557      0.196780  \n",
       "75%        3.155235      1.049781      1.068512      0.908975      1.822896  \n",
       "max       18.604384     15.941515     15.278376      7.844041     14.235675  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data = X).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Fpz8X5DMSvn2"
   },
   "outputs": [],
   "source": [
    "#reshaping y and weight vector for compatibilty\n",
    "# adding a 0 at the first position of w as the intial intercept for weight vector. this means w[0] is our intercept\n",
    "# adding a column of 1s to X. This will work as the compatibility term for our intercept.\n",
    "# now our eq is from y = w.T*X + b to y = w.T*X . \n",
    "# this concept has been taken from Andrew Ng's machine learning course\n",
    "\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "#X = np.hstack((np.ones_like(X[:,0]).reshape(-1,1),X))\n",
    "w = np.zeros_like(X[0]).reshape(-1,1)\n",
    "b = 0\n",
    "np.random.seed(0)\n",
    "#W = np.random.uniform(0,1,size=(X.shape[1],1))\n",
    "#b=0.5\n",
    "\n",
    "eta0  = 0.0001 # learning rate\n",
    "alpha = 0.0001 # regularizing term i.e. lambda\n",
    "N = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OAiKtwDXXCl9"
   },
   "outputs": [],
   "source": [
    "#Splitting X,y into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Gaom9khshRRa",
    "outputId": "8e77e853-9fab-4672-eabe-edf25ed1f866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.17133535 -1.00849691  0.40726112 -2.05334509 -1.37381592 -2.99724545\n",
      "  0.7787227   0.87207405 -2.17362041  1.22938588  0.21266735 -2.21599818\n",
      " -1.8801447  -0.61688062 -0.68442615]\n"
     ]
    }
   ],
   "source": [
    "print(X[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWeke9uYsunj"
   },
   "source": [
    "* All the value of weight vector have been initialized to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3PiQrZfNXLaO",
    "outputId": "e62f5506-f120-4efb-9994-94b9fe4d1265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 1)\n",
      "(50000, 15)\n",
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mH7jfIQ_9OEQ"
   },
   "outputs": [],
   "source": [
    "# Definition of sigmoid function used in Logistic Regression\n",
    "def sigmoid(X):\n",
    "   return 1/(1+np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UzmEy3mDgU_c"
   },
   "outputs": [],
   "source": [
    "#the below function provides the probabilistic prediction of a datapoint in X.\n",
    "def pred(X,w,b):\n",
    "  prediction = sigmoid(np.dot(X,w)+b)\n",
    "  for i in range(len(prediction)):\n",
    "    if prediction[i] == 1.0:\n",
    "      prediction[i] = sigmoid(36)\n",
    "  # shape is m * 1\n",
    "  return(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Uwt_YtRlibeN"
   },
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/multiplication-two-matrices-single-line-using-numpy-python/\n",
    "# https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#training\n",
    "# Below function calculates the cost/loss of a prediction\n",
    "# alpha is the regularization multiplyer here. i.e. lambda\n",
    "def cost_func(X,y,w,lamda,b):\n",
    "  y = y.reshape(-1,1)\n",
    "  #cost =  sum(np.log(1-np.exp(-y*(np.dot(X,w)))))\n",
    "  cost = (np.sum(-y*np.log(pred(X,w,b)) - (1-y)*np.log(1-pred(X,w,b))) + np.sum((lamda/2)*w**2))/len(y)\n",
    "  return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mtnh1V658t82",
    "outputId": "4221d23a-e39a-4f6e-b6e3-5433740e0245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69314718055994529]\n"
     ]
    }
   ],
   "source": [
    "train_cost_list = []\n",
    "test_cost_list = []\n",
    "train_cost_list.append(cost_func(X_train,y_train,w,alpha,b))\n",
    "print(train_cost_list) # Avg loss of training set when weight vector is 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9mz6vMAX_HCg",
    "outputId": "b1c2f311-3e14-4cb4-b0a9-48d0b43f5a4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69314718055994529"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for cross-checking\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "loss = log_loss(y, pred(X,w,b))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3eTZz0LJ_oXV"
   },
   "source": [
    "$w^{(t+1)} ← (1 − \\frac{αλ}{N} )w^{(t)} + αx_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))$ <br>\n",
    "        $b^{(t+1)} ← (b^t −  α(-y_n + σ((w^{(t)})^{T} x_n+b^{t}))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B6Y5kVscSvn5"
   },
   "outputs": [],
   "source": [
    "# derivative of cost function : https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression\n",
    "# below definition is used to optimize the lambda by subtracting its value for LR* (dJ/dw at w) .\n",
    "# lr is the learning rate at which we want our w to be optimized\n",
    "def optimized(X,y,w,lr,lamda,b):\n",
    "  # grad = y*np.log(pred(X,w)) + (1-y)*np.log(1-pred(X,w)) + alpha*np.dot(w.T,w)\n",
    "  #def_grad = (np.dot(X.T,(pred(X,w) - y)) + (lamda*2*w))/len(y)\n",
    "  #w = w - lr*def_grad\n",
    "  N = len(y)\n",
    "  #w = (1 - lr*lamda/N)*w + lr*np.dot(X.T,(y-pred(X,w,b)))\n",
    "  w = w - (lr/N)*(np.dot(X.T,(pred(X,w,b) - y)) + lamda*w)\n",
    "  b = b - lr*np.sum(pred(X,w,b) - y)/N\n",
    "  \n",
    "  return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "ODtttjBeibUk",
    "outputId": "b376a85b-0f7d-47bf-c7bd-b17b1e3f0c02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.404032239403\n",
      "1\n",
      "0.388407965583\n",
      "2\n",
      "0.383152713014\n",
      "3\n",
      "0.380790776947\n",
      "4\n",
      "0.379609356837\n",
      "5\n",
      "0.378987019198\n",
      "6\n",
      "0.378649671195\n",
      "7\n",
      "0.378463586214\n",
      "8\n",
      "0.37835973692\n",
      "9\n",
      "0.378301288038\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# since we dont know how many iteration it will take to find the optimized w which will give us the least loss,\n",
    "# we are using while loop with a stop condition\n",
    "# condition : when the last two loss values in the train_cost_list are very very close(~0.0000001) then we can stop the loop\n",
    "# We are using the X_train to optimize the w but we are storing lossvalues of train and test set to compare them.\n",
    "\n",
    "\n",
    "iter_ = True\n",
    "w,b = optimized(X_train,y_train,w,0.001,alpha,b)\n",
    "train_cost_list.append(cost_func(X_train,y_train,w,alpha,b))\n",
    "test_cost_list.append(cost_func(X_test,y_test,w,alpha,b))\n",
    "iter_count = 0\n",
    "while iter_:\n",
    "  for j in range(len(X_train)):\n",
    "    w,b = optimized(X_train[j:j+1,:],y_train[j],w,0.0001,alpha,b)\n",
    "\n",
    "  #row = random.sample(range(len(X_train)),50000)\n",
    "  #xx = X_train[row,:]\n",
    "  #yy = y_train[row]\n",
    "  #w,b = optimized(xx,yy,w,0.05,alpha,b)\n",
    " \n",
    "  tr_cost = cost_func(X_train,y_train,w,alpha,b)\n",
    "  te_cost = cost_func(X_test,y_test,w,alpha,b)\n",
    "\n",
    "  train_cost_list.append(tr_cost)\n",
    "  test_cost_list.append(te_cost)\n",
    "  iter_count += 1\n",
    "  if abs(train_cost_list[-1] - train_cost_list[-2]) < 0.0001:\n",
    "    iter_ = False\n",
    "  print(train_cost_list[-1])\n",
    "  print(iter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8xoK88whEW18",
    "outputId": "f8689ecd-0072-470b-b67e-88c7689d785b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 15)\n",
      "(15, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X.shape)\n",
    "print(w.shape)\n",
    "#print(b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MO-1hUUgT-Fe"
   },
   "outputs": [],
   "source": [
    "w = np.array(w).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gBkZdl1tqofj",
    "outputId": "185e34be-8c52-4be0-80ea-94a0f3b3a8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.378301288038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38029642263059155"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_cost_list[-1])\n",
    "test_cost_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "1YZoykX0Vu7Y",
    "outputId": "453ea7a2-5d0f-4b10-867e-f85e20235a10"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYdVNHk36AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1820bd8a9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting train and test loss\n",
    "\n",
    "plt.plot(train_cost_list,label = 'Train Loss')\n",
    "plt.plot(test_cost_list,label = 'Test Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.title(\"Train Loss Vs Test Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "xkT1KBJuLXhz",
    "outputId": "a7989af1-d625-42dd-c4df-fe548bcaf576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best weight : \n",
      " [[-0.42291962]\n",
      " [ 0.19086047]\n",
      " [-0.14572511]\n",
      " [ 0.33795115]\n",
      " [-0.21177796]\n",
      " [ 0.56492274]\n",
      " [-0.44521243]\n",
      " [-0.09166814]\n",
      " [ 0.21784835]\n",
      " [ 0.16962552]\n",
      " [ 0.19510543]\n",
      " [ 0.00228425]\n",
      " [-0.07775621]\n",
      " [ 0.33865727]\n",
      " [ 0.02215788]]\n",
      "best intercept : -0.850007138476\n"
     ]
    }
   ],
   "source": [
    "# the optimized weight vector and the intercept\n",
    "print('best weight : \\n',w)\n",
    "print('best intercept :',b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Yy8jWaa7Svn_",
    "outputId": "de69adb4-b90f-45f5-906e-8d796a347c4d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.41177431,  0.18416782, -0.13895073,  0.33572511, -0.18423237,\n",
       "          0.5494352 , -0.45213692, -0.08857465,  0.21536661,  0.17351757,\n",
       "          0.18480827,  0.00443463, -0.07033001,  0.33683181,  0.02004129]]),\n",
       " (1, 15),\n",
       " array([-0.76181561]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "clf.coef_, clf.coef_.shape, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparing both the weight vector of my model and of the sklearn model , we can see there isnt much difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "48gx6wQKSvoE",
    "outputId": "9e64422b-d40e-4788-e522-db6312c7be20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.955306666667\n",
      "0.95288\n"
     ]
    }
   ],
   "source": [
    "# Avg Accuracy of our prediction\n",
    "def pred_(w, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        if pred(X[i],w,b) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict).reshape(-1,1)\n",
    "print(1-np.sum(y_train - pred_(w,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred_(w,X_test))/len(X_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "spr00007@gmail.com_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
