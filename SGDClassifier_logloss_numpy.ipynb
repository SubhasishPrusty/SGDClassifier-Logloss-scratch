{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spr00007@gmail.com_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2S-uFqwSvmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUxLkBjISvmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xexp5GYNSvmz",
        "colab_type": "code",
        "outputId": "07eea558-f9c2-4457-8a71-a60a48228677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 15), (50000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54vJVc_KSvm9",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pKAn1-ASvm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r97pFTgrSvnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jykLIXZNSvnJ",
        "colab_type": "code",
        "outputId": "a190deab-5605-4786-b4f6-a59c9300ff77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((37500, 15), (37500,), (12500, 15), (12500,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0-M6oXASvnO",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sShoMeocSvnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm6wi8L2SvnU",
        "colab_type": "code",
        "outputId": "be306b64-dadd-4efd-ef20-619b5f7d3c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# alpha : float  lambda\n",
        "# Constant that multiplies the regularization term. \n",
        "\n",
        "# eta0 : double\n",
        "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
        "\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4WFoxgASvnc",
        "colab_type": "code",
        "outputId": "56d8a216-b3f2-415e-d57e-5a8b00c96ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "clf.fit(X=X_train, y=y_train)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
            "Total training time: 0.09 seconds.\n",
            "Convergence after 10 epochs took 0.09 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
              "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
              "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
              "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
              "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WaVxhGpSvnj",
        "colab_type": "code",
        "outputId": "4597c980-81af-40ad-ca1d-2ff327025a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "clf.coef_, clf.coef_.shape, clf.intercept_"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
              "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
              "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
              " (1, 15),\n",
              " array([-0.8531383]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su9e8fRLSvno",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcz5_UqCSvnq",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOBvEchCSvnr",
        "colab_type": "text"
      },
      "source": [
        "## Implement Logistc Regression with L2 regularization Using SGD: without using sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbn61rrXSvnt",
        "colab_type": "text"
      },
      "source": [
        "### Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bA5yR3Svnv",
        "colab_type": "text"
      },
      "source": [
        "- Load the datasets(train and test) into the respective arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7183hFBSvnv",
        "colab_type": "text"
      },
      "source": [
        "- Initialize the weight_vector and intercept term randomly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdLeFU0USvnx",
        "colab_type": "text"
      },
      "source": [
        "- Calculate the initlal log loss for the train and test data with the current weight and intercept and store it in a list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEVtAlO1Svny",
        "colab_type": "text"
      },
      "source": [
        "- for each epoch:\n",
        "    - for each batch of data points in train: (keep batch size=1)\n",
        "        - calculate the gradient of loss function w.r.t each weight in weight vector\n",
        "        - Calculate the gradient of the intercept <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
        "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
        "        $w^{(t+1)} ← (1 − \\frac{αλ}{N} )w^{(t)} + αx_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))$ <br>\n",
        "        $b^{(t+1)} ← (b^t −  α(-y_n + σ((w^{(t)})^{T} x_n+b^{t}))$ \n",
        "        - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
        "        - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
        "        you can stop the training\n",
        "        - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qmRH4UpSvny",
        "colab_type": "text"
      },
      "source": [
        "- Plot the train and test loss i.e on x-axis the epoch number, and on y-axis the loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbZf9p5gSvn1",
        "colab_type": "text"
      },
      "source": [
        "- <strong>GOAL</strong>: compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0voVRQyp_NHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import math\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hz5KIxr9X7dJ",
        "colab": {}
      },
      "source": [
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBoYVMOulyt9",
        "colab_type": "code",
        "outputId": "466a774d-241a-4f4b-b508-312e3349e133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "pd.DataFrame(data = X).describe()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "      <td>50000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.266454</td>\n",
              "      <td>0.207401</td>\n",
              "      <td>0.207484</td>\n",
              "      <td>-0.491013</td>\n",
              "      <td>-0.700210</td>\n",
              "      <td>-0.294138</td>\n",
              "      <td>0.273313</td>\n",
              "      <td>0.695944</td>\n",
              "      <td>-0.030006</td>\n",
              "      <td>0.006645</td>\n",
              "      <td>0.609432</td>\n",
              "      <td>-1.110552</td>\n",
              "      <td>-0.528438</td>\n",
              "      <td>-0.511179</td>\n",
              "      <td>0.231617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.126825</td>\n",
              "      <td>1.970479</td>\n",
              "      <td>1.874263</td>\n",
              "      <td>1.762062</td>\n",
              "      <td>1.649456</td>\n",
              "      <td>1.723327</td>\n",
              "      <td>2.050374</td>\n",
              "      <td>1.986726</td>\n",
              "      <td>4.385574</td>\n",
              "      <td>2.094178</td>\n",
              "      <td>4.066106</td>\n",
              "      <td>3.406985</td>\n",
              "      <td>2.612897</td>\n",
              "      <td>2.073009</td>\n",
              "      <td>2.525177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-8.360184</td>\n",
              "      <td>-8.791185</td>\n",
              "      <td>-7.383914</td>\n",
              "      <td>-7.692214</td>\n",
              "      <td>-7.729431</td>\n",
              "      <td>-7.750905</td>\n",
              "      <td>-8.661642</td>\n",
              "      <td>-7.848835</td>\n",
              "      <td>-23.924167</td>\n",
              "      <td>-8.678568</td>\n",
              "      <td>-17.056269</td>\n",
              "      <td>-18.299831</td>\n",
              "      <td>-13.340440</td>\n",
              "      <td>-8.971211</td>\n",
              "      <td>-12.804785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.162846</td>\n",
              "      <td>-1.090813</td>\n",
              "      <td>-1.084520</td>\n",
              "      <td>-1.675401</td>\n",
              "      <td>-1.805095</td>\n",
              "      <td>-1.430182</td>\n",
              "      <td>-1.103160</td>\n",
              "      <td>-0.607170</td>\n",
              "      <td>-2.989116</td>\n",
              "      <td>-1.427887</td>\n",
              "      <td>-1.924670</td>\n",
              "      <td>-3.311911</td>\n",
              "      <td>-2.213970</td>\n",
              "      <td>-1.899357</td>\n",
              "      <td>-1.422223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.241495</td>\n",
              "      <td>0.235629</td>\n",
              "      <td>0.183688</td>\n",
              "      <td>-0.543458</td>\n",
              "      <td>-0.705345</td>\n",
              "      <td>-0.354602</td>\n",
              "      <td>0.275627</td>\n",
              "      <td>0.705302</td>\n",
              "      <td>0.027739</td>\n",
              "      <td>-0.018822</td>\n",
              "      <td>0.635744</td>\n",
              "      <td>-1.163694</td>\n",
              "      <td>-0.598621</td>\n",
              "      <td>-0.497557</td>\n",
              "      <td>0.196780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.683281</td>\n",
              "      <td>1.501122</td>\n",
              "      <td>1.461999</td>\n",
              "      <td>0.656996</td>\n",
              "      <td>0.401761</td>\n",
              "      <td>0.784513</td>\n",
              "      <td>1.660565</td>\n",
              "      <td>2.002337</td>\n",
              "      <td>2.912834</td>\n",
              "      <td>1.382269</td>\n",
              "      <td>3.155235</td>\n",
              "      <td>1.049781</td>\n",
              "      <td>1.068512</td>\n",
              "      <td>0.908975</td>\n",
              "      <td>1.822896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>11.017564</td>\n",
              "      <td>8.879740</td>\n",
              "      <td>8.065992</td>\n",
              "      <td>7.255772</td>\n",
              "      <td>6.487590</td>\n",
              "      <td>7.808481</td>\n",
              "      <td>8.361272</td>\n",
              "      <td>10.323580</td>\n",
              "      <td>19.131734</td>\n",
              "      <td>10.206260</td>\n",
              "      <td>18.604384</td>\n",
              "      <td>15.941515</td>\n",
              "      <td>15.278376</td>\n",
              "      <td>7.844041</td>\n",
              "      <td>14.235675</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0             1   ...            13            14\n",
              "count  50000.000000  50000.000000  ...  50000.000000  50000.000000\n",
              "mean       0.266454      0.207401  ...     -0.511179      0.231617\n",
              "std        2.126825      1.970479  ...      2.073009      2.525177\n",
              "min       -8.360184     -8.791185  ...     -8.971211    -12.804785\n",
              "25%       -1.162846     -1.090813  ...     -1.899357     -1.422223\n",
              "50%        0.241495      0.235629  ...     -0.497557      0.196780\n",
              "75%        1.683281      1.501122  ...      0.908975      1.822896\n",
              "max       11.017564      8.879740  ...      7.844041     14.235675\n",
              "\n",
              "[8 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVdXGvtEojJf",
        "colab_type": "code",
        "outputId": "b782a381-2509-4907-9294-a4ad9cd74570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len([i for i in y if i == 1])/len(y)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpz8X5DMSvn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reshaping y and weight vector for compatibilty\n",
        "# adding a 0 at the first position of w as the intial intercept for weight vector. this means w[0] is our intercept\n",
        "# adding a column of 1s to X. This will work as the compatibility term for our intercept.\n",
        "# now our eq is from y = w.T*X + b to y = w.T*X . \n",
        "# this concept has been taken from Andrew Ng's machine learning course\n",
        "\n",
        "y = y.reshape(-1,1)\n",
        "\n",
        "#X = np.hstack((np.ones_like(X[:,0]).reshape(-1,1),X))\n",
        "w = np.zeros_like(X[0]).reshape(-1,1)\n",
        "\n",
        "np.random.seed(0)\n",
        "W = np.random.uniform(0,1,size=(X.shape[1],1))\n",
        "b=0.5\n",
        "\n",
        "eta0  = 0.0001 # learning rate\n",
        "alpha = 0.0001 # regularizing term i.e. lambda\n",
        "N = len(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OAiKtwDXXCl9",
        "colab": {}
      },
      "source": [
        "#Splitting X,y into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gaom9khshRRa",
        "colab_type": "code",
        "outputId": "8e77e853-9fab-4672-eabe-edf25ed1f866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(X[0,:])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.17133535 -1.00849691  0.40726112 -2.05334509 -1.37381592 -2.99724545\n",
            "  0.7787227   0.87207405 -2.17362041  1.22938588  0.21266735 -2.21599818\n",
            " -1.8801447  -0.61688062 -0.68442615]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWeke9uYsunj",
        "colab_type": "text"
      },
      "source": [
        "* All the value of weight vector have been initialized to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PiQrZfNXLaO",
        "colab_type": "code",
        "outputId": "e62f5506-f120-4efb-9994-94b9fe4d1265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(w.shape)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15, 1)\n",
            "(50000, 15)\n",
            "(50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH7jfIQ_9OEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definition of sigmoid function used in Logistic Regression\n",
        "def sigmoid(X):\n",
        "   return 1/(1+np.exp(-X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzmEy3mDgU_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the below function provides the probabilistic prediction of a datapoint in X.\n",
        "def pred(X,w,b):\n",
        "  prediction = sigmoid(np.dot(X,w)+b)\n",
        "  for i in range(len(prediction)):\n",
        "    if prediction[i] == 1.0:\n",
        "      prediction[i] = sigmoid(36)\n",
        "  # shape is m * 1\n",
        "  return(prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwt_YtRlibeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.geeksforgeeks.org/multiplication-two-matrices-single-line-using-numpy-python/\n",
        "# https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#training\n",
        "# Below function calculates the cost/loss of a prediction\n",
        "# alpha is the regularization multiplyer here. i.e. lambda\n",
        "def cost_func(X,y,w,lamda,b):\n",
        "  y = y.reshape(-1,1)\n",
        "  #cost =  sum(np.log(1-np.exp(-y*(np.dot(X,w)))))\n",
        "  cost = (np.sum(-y*np.log(pred(X,w,b)) - (1-y)*np.log(1-pred(X,w,b))) + np.sum((lamda/2)*w**2))/len(y)\n",
        "  return cost\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtnh1V658t82",
        "colab_type": "code",
        "outputId": "4221d23a-e39a-4f6e-b6e3-5433740e0245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_cost_list = []\n",
        "test_cost_list = []\n",
        "train_cost_list.append(cost_func(X_train,y_train,w,alpha,b))\n",
        "print(train_cost_list) # Avg loss of training set when weight vector is 0s."
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8230236508467732]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mz6vMAX_HCg",
        "colab_type": "code",
        "outputId": "b1c2f311-3e14-4cb4-b0a9-48d0b43f5a4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#for cross-checking\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "loss = log_loss(y, pred(X,w,b))\n",
        "loss"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8231169841801067"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eTZz0LJ_oXV",
        "colab_type": "text"
      },
      "source": [
        "$w^{(t+1)} ← (1 − \\frac{αλ}{N} )w^{(t)} + αx_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))$ <br>\n",
        "        $b^{(t+1)} ← (b^t −  α(-y_n + σ((w^{(t)})^{T} x_n+b^{t}))$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6Y5kVscSvn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# derivative of cost function : https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression\n",
        "# below definition is used to optimize the lambda by subtracting its value for LR* (dJ/dw at w) .\n",
        "# lr is the learning rate at which we want our w to be optimized\n",
        "def optimized(X,y,w,lr,lamda,b):\n",
        "  # grad = y*np.log(pred(X,w)) + (1-y)*np.log(1-pred(X,w)) + alpha*np.dot(w.T,w)\n",
        "  #def_grad = (np.dot(X.T,(pred(X,w) - y)) + (lamda*2*w))/len(y)\n",
        "  #w = w - lr*def_grad\n",
        "  N = len(y)\n",
        "  #w = (1 - lr*lamda/N)*w + lr*np.dot(X.T,(y-pred(X,w,b)))\n",
        "  w = w - (lr/N)*(np.dot(X.T,(pred(X,w,b) - y)) + lamda*w)\n",
        "  b = b - lr*np.sum(pred(X,w,b) - y)/N\n",
        "  \n",
        "  return w,b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODtttjBeibUk",
        "colab_type": "code",
        "outputId": "b376a85b-0f7d-47bf-c7bd-b17b1e3f0c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "# since we dont know how many iteration it will take to find the optimized w which will give us the least loss,\n",
        "# we are using while loop with a stop condition\n",
        "# condition : when the last two loss values in the train_cost_list are very very close(~0.0000001) then we can stop the loop\n",
        "# We are using the X_train to optimize the w but we are storing lossvalues of train and test set to compare them.\n",
        "from tqdm import tqdm \n",
        "\n",
        "iter_ = True\n",
        "w,b = optimized(X_train,y_train,w,0.001,alpha,b)\n",
        "train_cost_list.append(cost_func(X_train,y_train,w,alpha,b))\n",
        "test_cost_list.append(cost_func(X_test,y_test,w,alpha,b))\n",
        "iter_count = 0\n",
        "while iter_:\n",
        "  for j in range(len(X_train)):\n",
        "    w,b = optimized(X_train[j:j+1,:],y_train[j],w,0.0001,alpha,b)\n",
        "\n",
        "  #row = random.sample(range(len(X_train)),50000)\n",
        "  #xx = X_train[row,:]\n",
        "  #yy = y_train[row]\n",
        "  #w,b = optimized(xx,yy,w,0.05,alpha,b)\n",
        " \n",
        "  tr_cost = cost_func(X_train,y_train,w,alpha,b)\n",
        "  te_cost = cost_func(X_test,y_test,w,alpha,b)\n",
        "\n",
        "  train_cost_list.append(tr_cost)\n",
        "  test_cost_list.append(te_cost)\n",
        "  iter_count += 1\n",
        "  if abs(train_cost_list[-1] - train_cost_list[-2]) < 0.000001:\n",
        "    iter_ = False\n",
        "  print(train_cost_list[-1])\n",
        "  print(iter_count)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.42371782703791666\n",
            "1\n",
            "0.39800627145166295\n",
            "2\n",
            "0.38806777159040995\n",
            "3\n",
            "0.38336660845647674\n",
            "4\n",
            "0.38098372727892515\n",
            "5\n",
            "0.3797313958790109\n",
            "6\n",
            "0.3790579078899805\n",
            "7\n",
            "0.37868980842591105\n",
            "8\n",
            "0.3784861790373487\n",
            "9\n",
            "0.3783724716027518\n",
            "10\n",
            "0.37830849456722376\n",
            "11\n",
            "0.37827226783431356\n",
            "12\n",
            "0.37825163785070903\n",
            "13\n",
            "0.3782398262523533\n",
            "14\n",
            "0.3782330264297055\n",
            "15\n",
            "0.3782290885720741\n",
            "16\n",
            "0.37822679273066895\n",
            "17\n",
            "0.3782254436521609\n",
            "18\n",
            "0.3782246434867191\n",
            "19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xoK88whEW18",
        "colab_type": "code",
        "outputId": "f8689ecd-0072-470b-b67e-88c7689d785b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "print(X.shape)\n",
        "print(w.shape)\n",
        "#print(b.shape)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 15)\n",
            "(15, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO-1hUUgT-Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = np.array(w).reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBkZdl1tqofj",
        "colab_type": "code",
        "outputId": "185e34be-8c52-4be0-80ea-94a0f3b3a8de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_cost_list[-1])\n",
        "test_cost_list[-1]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3782246434867191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3802148961624491"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YZoykX0Vu7Y",
        "colab_type": "code",
        "outputId": "453ea7a2-5d0f-4b10-867e-f85e20235a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# plotting train and test loss\n",
        "\n",
        "plt.plot(train_cost_list,label = 'Train Loss')\n",
        "plt.plot(test_cost_list,label = 'Test Loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch Number\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.title(\"Train Loss Vs Test Loss\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkT1KBJuLXhz",
        "colab_type": "code",
        "outputId": "a7989af1-d625-42dd-c4df-fe548bcaf576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# the optimized weight vector and the intercept\n",
        "print('best weight : \\n',w)\n",
        "print('best intercept :',b)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best weight : \n",
            " [[-4.28773996e-01]\n",
            " [ 1.92691533e-01]\n",
            " [-1.48022081e-01]\n",
            " [ 3.37904475e-01]\n",
            " [-2.20005537e-01]\n",
            " [ 5.69066368e-01]\n",
            " [-4.45018751e-01]\n",
            " [-9.00689817e-02]\n",
            " [ 2.21265720e-01]\n",
            " [ 1.73205339e-01]\n",
            " [ 1.98217723e-01]\n",
            " [-2.51693428e-04]\n",
            " [-8.08605278e-02]\n",
            " [ 3.38885755e-01]\n",
            " [ 2.28994749e-02]]\n",
            "best intercept : -0.8871356130232607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Yy8jWaa7Svn_",
        "colab_type": "code",
        "outputId": "de69adb4-b90f-45f5-906e-8d796a347c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
        "clf.coef_, clf.coef_.shape, clf.intercept_"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
              "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
              "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
              " (1, 15),\n",
              " array([-0.8531383]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48gx6wQKSvoE",
        "colab_type": "code",
        "outputId": "9e64422b-d40e-4788-e522-db6312c7be20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Avg Accuracy of our prediction\n",
        "def pred_(w, X):\n",
        "    N = len(X)\n",
        "    predict = []\n",
        "    for i in range(N):\n",
        "        if pred(X[i],w,b) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
        "            predict.append(1)\n",
        "        else:\n",
        "            predict.append(0)\n",
        "    return np.array(predict).reshape(-1,1)\n",
        "print(1-np.sum(y_train - pred_(w,X_train))/len(X_train))\n",
        "print(1-np.sum(y_test  - pred_(w,X_test))/len(X_test))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.95264\n",
            "0.95032\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
